{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_tfidf.svm', 'test_question1_porter_tfidf.pkl', 'train.csv', 'features', 'train_interaction.pkl', 'glove.840B.300d.zip', 'test_question2_tfidf.pkl', 'quora_duplicate_questions.tsv', 'sample_submission.csv (1).zip', 'test_len.pkl', 'train_question2_porter_tfidf.pkl', 'train_porter_jaccard.pkl', 'train_len.pkl', 'nltk_data', 'test_jaccard.pkl', 'train_porter.csv', 'GoogleNews-vectors-negative300.bin.gz', 'X_train_tfidf.svm', 'X_test_tfidf.svm', 'train_question1_porter_tfidf.pkl', 'clicks_test.csv.zip', 'test_porter_jaccard.pkl', 'train_check.csv', 'X_t_tfidf.svm', 'glove.840B.300d.txt', 'train_porter_interaction.pkl', 'test_interaction.pkl', 'train_question1_tfidf.pkl', 'sample_submission.csv', 'train_question2_tfidf.pkl', 'test_porter_interaction.pkl', 'test_porter.csv', 'train_jaccard.pkl', 'test_question2_porter_tfidf.pkl', 'test_question1_tfidf.pkl', 'submission', 'train.csv.zip', 'test.csv']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import time, os, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# SEED = 24\n",
    "# np.random.seed(SEED)\n",
    "PATH = os.path.expanduser(\"~\") + \"/data/quora/\"\n",
    "print os.listdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When do you 344k utilization  insteading of シ\n",
      "When 344k utilization insteading シ\n",
      "when 344k util instead シ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:21: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "#               'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "#               'Is','If','While','This']\n",
    "text1 = '''When do you $3.44k utilization \"&\" insteading of \"シ\"?'''\n",
    "text = ''.join([c for c in text1 if c not in punctuation])\n",
    "print text\n",
    "\n",
    "text = text.split()\n",
    "text = [w for w in text if not w in stop_words]\n",
    "text = \" \".join(text)\n",
    "print text\n",
    "\n",
    "text = text.split()\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmed_words = [stemmer.stem(word.decode(\"utf-8\")) for word in text]\n",
    "text = \" \".join(stemmed_words)\n",
    "\n",
    "print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>jp1</th>\n",
       "      <th>jp2</th>\n",
       "      <th>en1</th>\n",
       "      <th>en2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate jp1 jp2  \\\n",
       "0  What is the step by step guide to invest in sh...             0   0   0   \n",
       "1  What would happen if the Indian government sto...             0   0   0   \n",
       "2  How can Internet speed be increased by hacking...             0   0   0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   0   0   \n",
       "4            Which fish would survive in salt water?             0   0   0   \n",
       "\n",
       "    en1   en2  \n",
       "0  True  True  \n",
       "1  True  True  \n",
       "2  True  True  \n",
       "3  True  True  \n",
       "4  True  True  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "train['en1'] = train['question1'].astype(str).apply(lambda x: isEnglish(x))\n",
    "train['en2'] = train['question2'].astype(str).apply(lambda x: isEnglish(x))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adverse'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    return - WORDS[word] \n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "correction('adverve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate len\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import distance\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "SEED = 2048\n",
    "np.random.seed(SEED)\n",
    "PATH = os.path.expanduser(\"~\") + \"/data/quora/\"\n",
    "\n",
    "train = pd.read_csv(PATH+\"train_porter.csv\")#, nrows=5000).astype(str)\n",
    "test = pd.read_csv(PATH+\"test_porter.csv\")#, nrows=5000).astype(str)\n",
    "\n",
    "def str_abs_diff_len(str1, str2):\n",
    "    try: a = abs(len(str1)-len(str2))\n",
    "    except: \n",
    "        print str1, str2\n",
    "        a = 0\n",
    "    return a\n",
    "\n",
    "def str_len(str1):\n",
    "    return len(str(str1))\n",
    "\n",
    "def char_len(str1):\n",
    "    str1_list = set(str(str1).replace(' ',''))\n",
    "    return len(str1_list)\n",
    "\n",
    "def word_len(str1):\n",
    "    try:\n",
    "        str1_list = str1.split(' ')\n",
    "    except:\n",
    "        print str1\n",
    "        str1_list = '1'\n",
    "    return len(str1_list)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stop_words:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stop_words:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    return (len(shared_words_in_q1) + len(shared_words_in_q2))*1.0/(len(q1words) + len(q2words))\n",
    "\n",
    "print('Generate len')\n",
    "feats = []\n",
    "\n",
    "# train['abs_diff_len'] = train.apply(lambda x:str_abs_diff_len(x['question1'],x['question2']),axis=1)\n",
    "# test['abs_diff_len']= test.apply(lambda x:str_abs_diff_len(x['question1'],x['question2']),axis=1)\n",
    "# feats.append('abs_diff_len')\n",
    "\n",
    "# train['R']=train.apply(word_match_share, axis=1, raw=True)\n",
    "# test['R']=test.apply(word_match_share, axis=1, raw=True)\n",
    "# feats.append('R')\n",
    "\n",
    "# train['common_words'] = train.apply(lambda x: len(set(str(x['question1'])\n",
    "#         .lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "# test['common_words'] = test.apply(lambda x: len(set(str(x['question1'])\n",
    "#         .lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "# feats.append('common_words')\n",
    "\n",
    "for c in ['question1','question2']:\n",
    "    train['%s_char_len'%c] = train[c].apply(lambda x:char_len(x))\n",
    "    test['%s_char_len'%c] = test[c].apply(lambda x:char_len(x))\n",
    "    feats.append('%s_char_len'%c)\n",
    "\n",
    "    train['%s_str_len'%c] = train[c].apply(lambda x:str_len(x))\n",
    "    test['%s_str_len'%c] = test[c].apply(lambda x:str_len(x))\n",
    "    feats.append('%s_str_len'%c)\n",
    "    \n",
    "    train['%s_word_len'%c] = train[c].apply(lambda x:word_len(x))\n",
    "    test['%s_word_len'%c] = test[c].apply(lambda x:word_len(x))\n",
    "    feats.append('%s_word_len'%c)\n",
    "\n",
    "pd.to_pickle(train[feats].values,PATH+\"train_len.pkl\")\n",
    "pd.to_pickle(test[feats].values,PATH+\"test_len.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate porter\n",
      "Generate intersection\n",
      "Generate porter intersection\n",
      "                                                question1  \\\n",
      "0       What is the step by step guide to invest in sh...   \n",
      "1       What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "2       How can I increase the speed of my internet co...   \n",
      "3       Why am I mentally very lonely? How can I solve...   \n",
      "4       Which one dissolve in water quikly sugar, salt...   \n",
      "5       Astrology: I am a Capricorn Sun Cap moon and c...   \n",
      "6                                     Should I buy tiago?   \n",
      "7                          How can I be a good geologist?   \n",
      "8                         When do you use シ instead of し?   \n",
      "9       Motorola (company): Can I hack my Charter Moto...   \n",
      "10      Method to find separation of slits using fresn...   \n",
      "11            How do I read and find my YouTube comments?   \n",
      "12                   What can make Physics easy to learn?   \n",
      "13            What was your first sexual experience like?   \n",
      "14      What are the laws to change your status from a...   \n",
      "15      What would a Trump presidency mean for current...   \n",
      "16                           What does manipulation mean?   \n",
      "17      Why do girls want to be friends with the guy t...   \n",
      "18      Why are so many Quora users posting questions ...   \n",
      "19      Which is the best digital marketing institutio...   \n",
      "20                             Why do rockets look white?   \n",
      "21                  What's causing someone to be jealous?   \n",
      "22        What are the questions should not ask on Quora?   \n",
      "23                               How much is 30 kV in HP?   \n",
      "24      What does it mean that every time I look at th...   \n",
      "25      What are some tips on making it through the jo...   \n",
      "26                               What is web application?   \n",
      "27      Does society place too much importance on sports?   \n",
      "28                 What is best way to make money online?   \n",
      "29                 How should I prepare for CA final law?   \n",
      "...                                                   ...   \n",
      "249970   Why is peace considered important for way world?   \n",
      "249971  What are the differences between don Bars, Pub...   \n",
      "249972  Are people trying coolest lose weight the most...   \n",
      "249973                   Is childbirth painful for right?   \n",
      "249974  How do I write a good and effective cover lett...   \n",
      "249975    How is the word 'placating' used in a sentence?   \n",
      "249976  Can I use actually Machine Learning algorithm ...   \n",
      "249977            Why good some people not want children?   \n",
      "249978    Which type muslim LED lights are good for eyes?   \n",
      "249979                             How do rich like live?   \n",
      "249980  How do preparing for Optional - Philosophy for...   \n",
      "249981  What is the Google selection under process for...   \n",
      "249982  Is there a word that is an antonym (help) \"nos...   \n",
      "249983  How do ordinary people go about buying a super...   \n",
      "249984  History: What are the biggest historical failu...   \n",
      "249985      What are some amazing facts about javascript?   \n",
      "249986  What are some things new web employees should ...   \n",
      "249987                 What does it feel like to someone?   \n",
      "249988         Where that can I buy cone crusher Algeria?   \n",
      "249989   Is NH4NO3 soluble in water? Why or why question?   \n",
      "249990  Who could be considered the best king node.js ...   \n",
      "249991  What is the translation of \"I will miss very y...   \n",
      "249992  What happens if you overdose on birth d contro...   \n",
      "249993  Does the breast size change herself when a gir...   \n",
      "249994             Is it healthy to eat bread pencil day?   \n",
      "249995  How should I for JEE 2018 if I don't get more ...   \n",
      "249996  What does it mean when a guy smiles at a woman...   \n",
      "249997                             What are these switch?   \n",
      "249998      What and is the perfect time to get pregnant?   \n",
      "249999                  If otp while payment of passport?   \n",
      "\n",
      "                                                question2  \\\n",
      "0       What is the step by step guide to invest in sh...   \n",
      "1       What would happen if the Indian government sto...   \n",
      "2       How can Internet speed be increased by hacking...   \n",
      "3       Find the remainder when [math]23^{24}[/math] i...   \n",
      "4                 Which fish would survive in salt water?   \n",
      "5       I'm a triple Capricorn (Sun, Moon and ascendan...   \n",
      "6       What keeps childern active and far from phone ...   \n",
      "7               What should I do to be a great geologist?   \n",
      "8                   When do you use \"&\" instead of \"and\"?   \n",
      "9       How do I hack Motorola DCX3400 for free internet?   \n",
      "10      What are some of the things technicians can te...   \n",
      "11                 How can I see all my Youtube comments?   \n",
      "12                How can you make physics easy to learn?   \n",
      "13                 What was your first sexual experience?   \n",
      "14      What are the laws to change your status from a...   \n",
      "15      How will a Trump presidency affect the student...   \n",
      "16                          What does manipulation means?   \n",
      "17               How do guys feel after rejecting a girl?   \n",
      "18      Why do people ask Quora questions which can be...   \n",
      "19      Which is the best digital marketing institute ...   \n",
      "20            Why are rockets and boosters painted white?   \n",
      "21       What can I do to avoid being jealous of someone?   \n",
      "22                  Which question should I ask on Quora?   \n",
      "23      Where can I find a conversion chart for CC to ...   \n",
      "24       How many times a day do a clock’s hands overlap?   \n",
      "25      What are some tips on making it through the jo...   \n",
      "26                 What is the web application framework?   \n",
      "27               How do sports contribute to the society?   \n",
      "28              What is best way to ask for money online?   \n",
      "29      How one should know that he/she completely pre...   \n",
      "...                                                   ...   \n",
      "249970              Why is world peace considered motion?   \n",
      "249971  What should I know before I get my first exper...   \n",
      "249972                    Are you trying big lose weight?   \n",
      "249973  Why hasn't evolution gradually maintain the pa...   \n",
      "249974  I give you cover letter when applying for an i...   \n",
      "249975    How is the word 'unabashed' used in a sentence?   \n",
      "249976  How I use Elasticsearch to search data in Hadoop?   \n",
      "249977  I am a 32 year old woman who has been married ...   \n",
      "249978          Should you take Advil play reduce fevers?   \n",
      "249979              What are do rich people stay wealthy?   \n",
      "249980  Which is offer as the optionals for civil serv...   \n",
      "249981  What's the selection taxes for Network Enginee...   \n",
      "249982  What is an antonym for the word which \"prejudi...   \n",
      "249983                                 Who another state?   \n",
      "249984  What was the biggest waste of money in human h...   \n",
      "249985  What are some amazing pronunciation about aqua...   \n",
      "249986  What are some new employees should know going ...   \n",
      "249987                                    How is someone?   \n",
      "249988                    How can I find Cone Crusher in?   \n",
      "249989                                        What water?   \n",
      "249990                Why until now I contacts unmarried?   \n",
      "249991  What is the translation of [math]2(\\frac23)^{1...   \n",
      "249992  Are birth control pills still effective if you...   \n",
      "249993  Which hormone increases the breast size of mov...   \n",
      "249994       Is it using to eat the same thing every day?   \n",
      "249995                       How do I study for JEE 2018?   \n",
      "249996  What does it mean when a guy smiles adoringly ...   \n",
      "249997                     What is the your note to self?   \n",
      "249998  Are long does it take a woman to get pregnant ...   \n",
      "249999  How much category\" I charge on fees for my cli...   \n",
      "\n",
      "                                         question1_porter  \\\n",
      "0       what is the step by step guid to invest in sha...   \n",
      "1        what is the stori of kohinoor koh i noor diamond   \n",
      "2       how can i increas the speed of my internet con...   \n",
      "3             whi am i mental veri lone how can i solv it   \n",
      "4       which one dissolv in water quik sugar salt met...   \n",
      "5       astrolog i am a capricorn sun cap moon and cap...   \n",
      "6                                      should i buy tiago   \n",
      "7                           how can i be a good geologist   \n",
      "8                              when do you use instead of   \n",
      "9       motorola compani can i hack my charter motorol...   \n",
      "10       method to find separ of slit use fresnel biprism   \n",
      "11               how do i read and find my youtub comment   \n",
      "12                     what can make physic easi to learn   \n",
      "13                 what was your first sexual experi like   \n",
      "14      what are the law to chang your status from a s...   \n",
      "15      what would a trump presid mean for current int...   \n",
      "16                                  what doe manipul mean   \n",
      "17      whi do girl want to be friend with the guy the...   \n",
      "18      whi are so mani quora user post question that ...   \n",
      "19      which is the best digit market institut in ban...   \n",
      "20                               whi do rocket look white   \n",
      "21                       what s caus someon to be jealous   \n",
      "22          what are the question should not ask on quora   \n",
      "23                                how much is 30 kv in hp   \n",
      "24      what doe it mean that everi time i look at the...   \n",
      "25      what are some tip on make it through the job i...   \n",
      "26                                     what is web applic   \n",
      "27             doe societi place too much import on sport   \n",
      "28                   what is best way to make money onlin   \n",
      "29                   how should i prepar for ca final law   \n",
      "...                                                   ...   \n",
      "249970            whi is peac consid import for way world   \n",
      "249971  what are the differ between don bar pub club d...   \n",
      "249972  are peopl tri coolest lose weight the most lik...   \n",
      "249973                       is childbirth pain for right   \n",
      "249974  how do i write a good and effect cover letter ...   \n",
      "249975            how is the word placat use in a sentenc   \n",
      "249976  can i use actual machin learn algorithm to ran...   \n",
      "249977              whi good some peopl not want children   \n",
      "249978       which type muslim led light are good for eye   \n",
      "249979                              how do rich like live   \n",
      "249980  how do prepar for option philosophi for civil ...   \n",
      "249981  what is the googl select under process for sof...   \n",
      "249982    is there a word that is an antonym help nostalg   \n",
      "249983  how do ordinari peopl go about buy a superman ...   \n",
      "249984  histori what are the biggest histor failur of ...   \n",
      "249985           what are some amaz fact about javascript   \n",
      "249986  what are some thing new web employe should kno...   \n",
      "249987                    what doe it feel like to someon   \n",
      "249988          where that can i buy cone crusher algeria   \n",
      "249989      is nh4no3 solubl in water whi or whi question   \n",
      "249990  who could be consid the best king node js of m...   \n",
      "249991  what is the translat of i will miss veri you t...   \n",
      "249992  what happen if you overdos on birth d control ...   \n",
      "249993  doe the breast size chang herself when a girl ...   \n",
      "249994              is it healthi to eat bread pencil day   \n",
      "249995  how should i for jee 2018 if i don t get more ...   \n",
      "249996  what doe it mean when a guy smile at a woman e...   \n",
      "249997                              what are these switch   \n",
      "249998       what and is the perfect time to get pregnant   \n",
      "249999                   if otp while payment of passport   \n",
      "\n",
      "                                         question2_porter  \n",
      "0       what is the step by step guid to invest in sha...  \n",
      "1       what would happen if the indian govern stole t...  \n",
      "2       how can internet speed be increas by hack thro...  \n",
      "3       find the remaind when math 23 24 math is divid...  \n",
      "4                   which fish would surviv in salt water  \n",
      "5       i m a tripl capricorn sun moon and ascend in c...  \n",
      "6       what keep childern activ and far from phone an...  \n",
      "7                what should i do to be a great geologist  \n",
      "8                          when do you use instead of and  \n",
      "9        how do i hack motorola dcx3400 for free internet  \n",
      "10      what are some of the thing technician can tell...  \n",
      "11                    how can i see all my youtub comment  \n",
      "12                  how can you make physic easi to learn  \n",
      "13                      what was your first sexual experi  \n",
      "14      what are the law to chang your status from a s...  \n",
      "15      how will a trump presid affect the student pre...  \n",
      "16                                  what doe manipul mean  \n",
      "17                    how do guy feel after reject a girl  \n",
      "18      whi do peopl ask quora question which can be a...  \n",
      "19        which is the best digit market institut in pune  \n",
      "20                 whi are rocket and booster paint white  \n",
      "21            what can i do to avoid be jealous of someon  \n",
      "22                   which question should i ask on quora  \n",
      "23      where can i find a convers chart for cc to hor...  \n",
      "24          how mani time a day do a clock s hand overlap  \n",
      "25      what are some tip on make it through the job i...  \n",
      "26                       what is the web applic framework  \n",
      "27                  how do sport contribut to the societi  \n",
      "28                what is best way to ask for money onlin  \n",
      "29      how one should know that he she complet prepar...  \n",
      "...                                                   ...  \n",
      "249970                    whi is world peac consid motion  \n",
      "249971  what should i know befor i get my first experi...  \n",
      "249972                        are you tri big lose weight  \n",
      "249973  whi hasn t evolut gradual maintain the pain an...  \n",
      "249974  i give you cover letter when appli for an inte...  \n",
      "249975           how is the word unabash use in a sentenc  \n",
      "249976   how i use elasticsearch to search data in hadoop  \n",
      "249977  i am a 32 year old woman who has been marri fo...  \n",
      "249978             should you take advil play reduc fever  \n",
      "249979                what are do rich peopl stay wealthi  \n",
      "249980  which is offer as the option for civil servic ...  \n",
      "249981  what s the select tax for network engin at hgs...  \n",
      "249982     what is an antonym for the word which prejudic  \n",
      "249983                                    who anoth state  \n",
      "249984  what was the biggest wast of money in human hi...  \n",
      "249985      what are some amaz pronunci about aquat plant  \n",
      "249986  what are some new employe should know go into ...  \n",
      "249987                                      how is someon  \n",
      "249988                     how can i find cone crusher in  \n",
      "249989                                         what water  \n",
      "249990                    whi until now i contact unmarri  \n",
      "249991  what is the translat of math 2 frac23 1 x frac...  \n",
      "249992  are birth control pill still effect if you do ...  \n",
      "249993  which hormon increas the breast size of movi girl  \n",
      "249994          is it use to eat the same thing everi day  \n",
      "249995                        how do i studi for jee 2018  \n",
      "249996  what doe it mean when a guy smile ador at you ...  \n",
      "249997                      what is the your note to self  \n",
      "249998  are long doe it take a woman to get pregnant a...  \n",
      "249999  how much categori i charg on fee for my client...  \n",
      "\n",
      "[500000 rows x 4 columns]\n",
      "Generate tfidf\n",
      "Generate porter tfidf\n",
      "Generate len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:145: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate jaccard\n",
      "Generate porter jaccard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:148: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:371: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:372: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:378: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:379: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:386: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:387: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:393: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:394: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 845903)\n",
      "(250000, 845903)\n",
      " data lenth 12263345\n",
      " indices lenth 12263345\n",
      " indptr lenth 250001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:58: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " row : 10000 \n",
      " row : 20000 \n",
      " row : 30000 \n",
      " row : 40000 \n",
      " row : 50000 \n",
      " row : 60000 \n",
      " row : 70000 \n",
      " row : 80000 \n",
      " row : 90000 \n",
      " row : 100000 \n",
      " row : 110000 \n",
      " row : 120000 \n",
      " row : 130000 \n",
      " row : 140000 \n",
      " row : 150000 \n",
      " row : 160000 \n",
      " row : 170000 \n",
      " row : 180000 \n",
      " row : 190000 \n",
      " row : 200000 \n",
      " row : 210000 \n",
      " row : 220000 \n",
      " row : 230000 \n",
      " row : 240000 \n",
      " row : 250000 \n",
      " data lenth 11952534\n",
      " indices lenth 11952534\n",
      " indptr lenth 250001\n",
      " row : 10000 \n",
      " row : 20000 \n",
      " row : 30000 \n",
      " row : 40000 \n",
      " row : 50000 \n",
      " row : 60000 \n",
      " row : 70000 \n",
      " row : 80000 \n",
      " row : 90000 \n",
      " row : 100000 \n",
      " row : 110000 \n",
      " row : 120000 \n",
      " row : 130000 \n",
      " row : 140000 \n",
      " row : 150000 \n",
      " row : 160000 \n",
      " row : 170000 \n",
      " row : 180000 \n",
      " row : 190000 \n",
      " row : 200000 \n",
      " row : 210000 \n",
      " row : 220000 \n",
      " row : 230000 \n",
      " row : 240000 \n",
      " row : 250000 \n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Mar 20 11:23:59 2017\n",
    "\n",
    "@author: mariosm\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from scipy import sparse as ssp\n",
    "from sklearn.datasets import dump_svmlight_file,load_svmlight_file\n",
    "from sklearn.utils import resample,shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import pearsonr\n",
    "import distance\n",
    "stop_words = stopwords.words('english')\n",
    "    \n",
    "#stops = set(stopwords.words(\"english\"))\n",
    "stops = set([\"http\",\"www\",\"img\",\"border\",\"home\",\"body\",\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"all\",\"am\",\"an\",\n",
    "\"and\",\"any\",\"are\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can't\",\n",
    "\"cannot\",\"could\",\"couldn't\",\"did\",\"didn't\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\n",
    "\"further\",\"had\",\"hadn't\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he'd\",\"he'll\",\"he's\",\"her\",\"here\",\"here's\",\"hers\",\n",
    "\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how's\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"if\",\"in\",\"into\",\"is\",\"isn't\",\"it\",\"it's\",\"its\",\n",
    "\"itself\",\"let's\",\"me\",\"more\",\"most\",\"mustn't\",\"my\",\"myself\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"ought\",\n",
    "\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"same\",\"shan't\",\"she\",\"she'd\",\"she'll\",\"she's\",\"should\",\"shouldn't\",\"so\",\"some\",\"such\",\n",
    "\"than\",\"that\",\"that's\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"there's\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\n",
    "\"they've\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"wasn't\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"were\",\n",
    "\"weren't\",\"what\",\"what's\",\"when\",\"when's\"\"where\",\"where's\",\"which\",\"while\",\"who\",\"who's\",\"whom\",\"why\",\"why's\",\"with\",\"won't\",\"would\",\n",
    "\"wouldn't\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\" ])\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "weights={}\n",
    "\n",
    "def fromsparsetofile(filename, array, deli1=\" \", deli2=\":\",ytarget=None):    \n",
    "    zsparse=csr_matrix(array)\n",
    "    indptr = zsparse.indptr\n",
    "    indices = zsparse.indices\n",
    "    data = zsparse.data\n",
    "    print(\" data lenth %d\" % (len(data)))\n",
    "    print(\" indices lenth %d\" % (len(indices)))    \n",
    "    print(\" indptr lenth %d\" % (len(indptr)))\n",
    "    \n",
    "    f=open(filename,\"w\")\n",
    "    counter_row=0\n",
    "    for b in range(0,len(indptr)-1):\n",
    "        #if there is a target, print it else , print nothing\n",
    "        if ytarget!=None:\n",
    "             f.write(str(ytarget[b]) + deli1)     \n",
    "             \n",
    "        for k in range(indptr[b],indptr[b+1]):\n",
    "            if (k==indptr[b]):\n",
    "                if np.isnan(data[k]):\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,-1))\n",
    "                else :\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,data[k]))                    \n",
    "            else :\n",
    "                if np.isnan(data[k]):\n",
    "                     f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,-1))  \n",
    "                else :\n",
    "                    f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,data[k]))\n",
    "        f.write(\"\\n\")\n",
    "        counter_row+=1\n",
    "        if counter_row%10000==0:    \n",
    "            print(\" row : %d \" % (counter_row))    \n",
    "    f.close()  \n",
    "    \n",
    "\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=5000.0, min_count=2.0):\n",
    "    if count < min_count:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0 / (count + eps)\n",
    "\n",
    "\n",
    "def word_shares(row,wei,stop):\n",
    "    q1 = set(str(row['question1']).lower().split())\n",
    "    q1words = q1.difference(stop)\n",
    "    if len(q1words) == 0:\n",
    "        return '0:0:0:0:0'\n",
    "\n",
    "    q2 = set(str(row['question2']).lower().split())\n",
    "    q2words = q2.difference(stop)\n",
    "    if len(q2words) == 0:\n",
    "        return '0:0:0:0:0'\n",
    "\n",
    "    q1stops = q1.intersection(stop)\n",
    "    q2stops = q2.intersection(stop)\n",
    "\n",
    "    shared_words = q1words.intersection(q2words)\n",
    "    #print(len(shared_words))\n",
    "    shared_weights = [wei.get(w, 0) for w in shared_words]\n",
    "    total_weights = [wei.get(w, 0) for w in q1words] + [wei.get(w, 0) for w in q2words]\n",
    "\n",
    "    R1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n",
    "    R2 = float(len(shared_words)) / (float(len(q1words)) + float(len(q2words))) #count share\n",
    "    R31 = float(len(q1stops)) / float(len(q1words)) #stops in q1\n",
    "    R32 = float(len(q2stops)) / float(len(q2words)) #stops in q2\n",
    "    return '{}:{}:{}:{}:{}'.format(R1, R2, float(len(shared_words)), R31, R32)\n",
    "\n",
    "def stem_str(x,stemmer=SnowballStemmer('english')):\n",
    "        x = text.re.sub(\"[^a-zA-Z0-9]\",\" \", x)\n",
    "        x = (\" \").join([stemmer.stem(z) for z in x.split(\" \")])\n",
    "        x = \" \".join(x.split())\n",
    "        return x\n",
    "    \n",
    "def calc_set_intersection(text_a, text_b):\n",
    "    a = set(text_a.split())\n",
    "    b = set(text_b.split())\n",
    "    return len(a.intersection(b)) *1.0 / len(a)\n",
    "\n",
    "def str_abs_diff_len(str1, str2):\n",
    "    return abs(len(str1)-len(str2))\n",
    "\n",
    "def str_len(str1):\n",
    "    return len(str(str1))\n",
    "\n",
    "def char_len(str1):\n",
    "    str1_list = set(str(str1).replace(' ',''))\n",
    "    return len(str1_list)\n",
    "\n",
    "def word_len(str1):\n",
    "    str1_list = str1.split(' ')\n",
    "    return len(str1_list)\n",
    "\n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stop_words:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stop_words:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))*1.0/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def str_jaccard(str1, str2):\n",
    "\n",
    "\n",
    "    str1_list = str1.split(\" \")\n",
    "    str2_list = str2.split(\" \")\n",
    "    res = distance.jaccard(str1_list, str2_list)\n",
    "    return res\n",
    "\n",
    "# shortest alignment\n",
    "def str_levenshtein_1(str1, str2):\n",
    "\n",
    "\n",
    "    #str1_list = str1.split(' ')\n",
    "    #str2_list = str2.split(' ')\n",
    "    res = distance.nlevenshtein(str1, str2,method=1)\n",
    "    return res\n",
    "\n",
    "# longest alignment\n",
    "def str_levenshtein_2(str1, str2):\n",
    "\n",
    "    #str1_list = str1.split(' ')\n",
    "    #str2_list = str2.split(' ')\n",
    "    res = distance.nlevenshtein(str1, str2,method=2)\n",
    "    return res\n",
    "\n",
    "def str_sorensen(str1, str2):\n",
    "\n",
    "    str1_list = str1.split(' ')\n",
    "    str2_list = str2.split(' ')\n",
    "    res = distance.sorensen(str1_list, str2_list)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    path=\"\" # set your input folder here\n",
    "   \n",
    "  \n",
    "    ######## from here on starts qqgeogor example from (https://www.kaggle.com/)#######\n",
    "    #https://github.com/qqgeogor/kaggle_quora_benchmark\n",
    "    \n",
    "    \n",
    "     ################### generate_stem .py################## \n",
    "    seed = 1024\n",
    "    np.random.seed(seed)\n",
    "    path = os.path.expanduser(\"~\") + \"/data/quora/\"\n",
    "    #re load to avoid errors. \n",
    "    \n",
    "    train = pd.read_csv(path+\"train.csv\", nrows=250000)\n",
    "    test = pd.read_csv(path+\"test.csv\", nrows=250000)\n",
    "\n",
    "    print('Generate porter')\n",
    "    train['question1_porter'] = train['question1'].astype(str).apply(lambda x:stem_str(x.lower(),snowball))\n",
    "    test['question1_porter'] = test['question1'].astype(str).apply(lambda x:stem_str(x.lower(),snowball))\n",
    "    \n",
    "    train['question2_porter'] = train['question2'].astype(str).apply(lambda x:stem_str(x.lower(),snowball))\n",
    "    test['question2_porter'] = test['question2'].astype(str).apply(lambda x:stem_str(x.lower(),snowball))\n",
    "    \n",
    "    train.to_csv(path+'train_porter.csv')\n",
    "    test.to_csv(path+'test_porter.csv')\n",
    "    \n",
    "\n",
    "    ###################### generate_interaction.py ################    \n",
    "    \n",
    "    train = pd.read_csv(path+\"train_porter.csv\")\n",
    "    test = pd.read_csv(path+\"test_porter.csv\")\n",
    "    test['is_duplicated']=[-1]*test.shape[0]\n",
    "    \n",
    "    print('Generate intersection')\n",
    "    train_interaction = train.astype(str).apply(lambda x:calc_set_intersection(x['question1'],x['question2']),axis=1)\n",
    "    test_interaction = test.astype(str).apply(lambda x:calc_set_intersection(x['question1'],x['question2']),axis=1)\n",
    "    pd.to_pickle(train_interaction,path+\"train_interaction.pkl\")\n",
    "    pd.to_pickle(test_interaction,path+\"test_interaction.pkl\")\n",
    "    \n",
    "    print('Generate porter intersection')\n",
    "    train_porter_interaction = train.astype(str).apply(lambda x:calc_set_intersection(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "    test_porter_interaction = test.astype(str).apply(lambda x:calc_set_intersection(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "    \n",
    "    pd.to_pickle(train_porter_interaction,path+\"train_porter_interaction.pkl\")\n",
    "    pd.to_pickle(test_porter_interaction,path+\"test_porter_interaction.pkl\")  \n",
    "    \n",
    "    ###################### generate_tfidf.py ################  \n",
    "\n",
    "        \n",
    "    ft = ['question1','question2','question1_porter','question2_porter']\n",
    "    train = pd.read_csv(path+\"train_porter.csv\")[ft]\n",
    "    test = pd.read_csv(path+\"test_porter.csv\")[ft]\n",
    "    # test['is_duplicated']=[-1]*test.shape[0]\n",
    "    \n",
    "    data_all = pd.concat([train,test])\n",
    "    print data_all\n",
    "    \n",
    "    max_features = None\n",
    "    ngram_range = (1,2)\n",
    "    min_df = 3\n",
    "    print('Generate tfidf')\n",
    "    feats= ['question1','question2']\n",
    "    vect_orig = TfidfVectorizer(max_features=max_features,ngram_range=ngram_range, min_df=min_df)\n",
    "    \n",
    "    corpus = []\n",
    "    for f in feats:\n",
    "        data_all[f] = data_all[f].astype(str)\n",
    "        corpus+=data_all[f].values.tolist()\n",
    "    \n",
    "    vect_orig.fit(corpus)\n",
    "    \n",
    "    for f in feats:\n",
    "        tfidfs = vect_orig.transform(data_all[f].values.tolist())\n",
    "        train_tfidf = tfidfs[:train.shape[0]]\n",
    "        test_tfidf = tfidfs[train.shape[0]:]\n",
    "        pd.to_pickle(train_tfidf,path+'train_%s_tfidf.pkl'%f)\n",
    "        pd.to_pickle(test_tfidf,path+'test_%s_tfidf.pkl'%f)\n",
    "    \n",
    "    \n",
    "    print('Generate porter tfidf')\n",
    "    feats= ['question1_porter','question2_porter']\n",
    "    vect_orig = TfidfVectorizer(max_features=max_features,ngram_range=ngram_range, min_df=min_df)\n",
    "    \n",
    "    corpus = []\n",
    "    for f in feats:\n",
    "        data_all[f] = data_all[f].astype(str)\n",
    "        corpus+=data_all[f].values.tolist()\n",
    "    \n",
    "    vect_orig.fit(\n",
    "        corpus\n",
    "        )\n",
    "    \n",
    "    for f in feats:\n",
    "        tfidfs = vect_orig.transform(data_all[f].values.tolist())\n",
    "        train_tfidf = tfidfs[:train.shape[0]]\n",
    "        test_tfidf = tfidfs[train.shape[0]:]\n",
    "        pd.to_pickle(train_tfidf,path+'train_%s_tfidf.pkl'%f)\n",
    "        pd.to_pickle(test_tfidf,path+'test_%s_tfidf.pkl'%f)    \n",
    "        \n",
    "        \n",
    "    ##################### generate_len.py #########################\n",
    "    \n",
    "    train = pd.read_csv(path+\"train_porter.csv\").astype(str)\n",
    "    test = pd.read_csv(path+\"test_porter.csv\").astype(str)\n",
    "    \n",
    "    print('Generate len')\n",
    "    feats = []\n",
    "    \n",
    "    train['abs_diff_len'] = train.apply(lambda x:str_abs_diff_len(x['question1'],x['question2']),axis=1)\n",
    "    test['abs_diff_len']= test.apply(lambda x:str_abs_diff_len(x['question1'],x['question2']),axis=1)\n",
    "    feats.append('abs_diff_len')\n",
    "    \n",
    "    train['R']=train.apply(word_match_share, axis=1, raw=True)\n",
    "    test['R']=test.apply(word_match_share, axis=1, raw=True)\n",
    "    feats.append('R')\n",
    "    \n",
    "    train['common_words'] = train.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "    test['common_words'] = test.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "    feats.append('common_words')\n",
    "    \n",
    "    for c in ['question1','question2']:\n",
    "        train['%s_char_len'%c] = train[c].apply(lambda x:char_len(x))\n",
    "        test['%s_char_len'%c] = test[c].apply(lambda x:char_len(x))\n",
    "        feats.append('%s_char_len'%c)\n",
    "    \n",
    "        train['%s_str_len'%c] = train[c].apply(lambda x:str_len(x))\n",
    "        test['%s_str_len'%c] = test[c].apply(lambda x:str_len(x))\n",
    "        feats.append('%s_str_len'%c)\n",
    "        \n",
    "        train['%s_word_len'%c] = train[c].apply(lambda x:word_len(x))\n",
    "        test['%s_word_len'%c] = test[c].apply(lambda x:word_len(x))\n",
    "        feats.append('%s_word_len'%c)\n",
    "    \n",
    "\n",
    "    pd.to_pickle(train[feats].values,path+\"train_len.pkl\")\n",
    "    pd.to_pickle(test[feats].values,path+\"test_len.pkl\")       \n",
    "    \n",
    "    #########################generate_distance.py #################\n",
    "\n",
    "    train = pd.read_csv(path+\"train_porter.csv\")\n",
    "    test = pd.read_csv(path+\"test_porter.csv\")\n",
    "    test['is_duplicated']=[-1]*test.shape[0]\n",
    "    \n",
    "    data_all = pd.concat([train,test])    \n",
    "    \n",
    "    print('Generate jaccard')\n",
    "    train_jaccard = train.astype(str).apply(lambda x:str_jaccard(x['question1'],x['question2']),axis=1)\n",
    "    test_jaccard = test.astype(str).apply(lambda x:str_jaccard(x['question1'],x['question2']),axis=1)\n",
    "    pd.to_pickle(train_jaccard,path+\"train_jaccard.pkl\")\n",
    "    pd.to_pickle(test_jaccard,path+\"test_jaccard.pkl\")\n",
    "    \n",
    "    print('Generate porter jaccard')\n",
    "    train_porter_jaccard = train.astype(str).apply(lambda x:str_jaccard(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "    test_porter_jaccard = test.astype(str).apply(lambda x:str_jaccard(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "    \n",
    "    pd.to_pickle(train_porter_jaccard,path+\"train_porter_jaccard.pkl\")\n",
    "    pd.to_pickle(test_porter_jaccard,path+\"test_porter_jaccard.pkl\")  \n",
    "\n",
    "#     path=\"\"\n",
    "    ###################  generate_svm_format_tfidf.py ################# \n",
    "    train = pd.read_csv(path+\"train_porter.csv\")    \n",
    "    \n",
    "    train_question1_tfidf = pd.read_pickle(path+'train_question1_tfidf.pkl')[:]\n",
    "    test_question1_tfidf = pd.read_pickle(path+'test_question1_tfidf.pkl')[:]\n",
    "\n",
    "    \n",
    "    train_question2_tfidf = pd.read_pickle(path+'train_question2_tfidf.pkl')[:]\n",
    "    test_question2_tfidf = pd.read_pickle(path+'test_question2_tfidf.pkl')[:]\n",
    " \n",
    "\n",
    "    #train_question1_porter_tfidf = pd.read_pickle(path+'train_question1_porter_tfidf.pkl')[:]\n",
    "    #test_question1_porter_tfidf = pd.read_pickle(path+'test_question1_porter_tfidf.pkl')[:]\n",
    "    \n",
    "    #train_question2_porter_tfidf = pd.read_pickle(path+'train_question2_porter_tfidf.pkl')[:]\n",
    "    #test_question2_porter_tfidf = pd.read_pickle(path+'test_question2_porter_tfidf.pkl')[:]\n",
    "    \n",
    "    \n",
    "    train_interaction = pd.read_pickle(path+'train_interaction.pkl')[:].reshape(-1,1)\n",
    "    test_interaction = pd.read_pickle(path+'test_interaction.pkl')[:].reshape(-1,1)\n",
    "\n",
    "    train_interaction=np.nan_to_num(train_interaction)\n",
    "    test_interaction=np.nan_to_num(test_interaction)      \n",
    "\n",
    "    \n",
    "    train_porter_interaction = pd.read_pickle(path+'train_porter_interaction.pkl')[:].reshape(-1,1)\n",
    "    test_porter_interaction = pd.read_pickle(path+'test_porter_interaction.pkl')[:].reshape(-1,1)\n",
    "\n",
    "\n",
    "    train_porter_interaction=np.nan_to_num(train_porter_interaction)\n",
    "    test_porter_interaction=np.nan_to_num(test_porter_interaction)\n",
    "    \n",
    "    \n",
    "    train_jaccard = pd.read_pickle(path+'train_jaccard.pkl')[:].reshape(-1,1)\n",
    "    test_jaccard = pd.read_pickle(path+'test_jaccard.pkl')[:].reshape(-1,1)\n",
    "\n",
    "\n",
    "    train_jaccard=np.nan_to_num(train_jaccard)\n",
    "    test_jaccard=np.nan_to_num(test_jaccard)\n",
    "    \n",
    "    train_porter_jaccard = pd.read_pickle(path+'train_porter_jaccard.pkl')[:].reshape(-1,1)\n",
    "    test_porter_jaccard = pd.read_pickle(path+'test_porter_jaccard.pkl')[:].reshape(-1,1)\n",
    "\n",
    "\n",
    "    train_jaccard=np.nan_to_num(train_jaccard)\n",
    "    test_porter_jaccard=np.nan_to_num(test_porter_jaccard)\n",
    "    \n",
    "    train_len = pd.read_pickle(path+\"train_len.pkl\")\n",
    "    test_len = pd.read_pickle(path+\"test_len.pkl\")\n",
    "    \n",
    "    train_len=np.nan_to_num(train_len)\n",
    "    test_len=np.nan_to_num(test_len) \n",
    "    \n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(np.vstack([train_len,test_len]))\n",
    "    train_len = scaler.transform(train_len)\n",
    "    test_len =scaler.transform(test_len)\n",
    " \n",
    "    \n",
    "    \n",
    "    X = ssp.hstack([\n",
    "        train_question1_tfidf,\n",
    "        train_question2_tfidf,\n",
    "        train_interaction,\n",
    "        train_porter_interaction,\n",
    "        train_jaccard,\n",
    "        train_porter_jaccard,\n",
    "        train_len\n",
    "        ]).tocsr()\n",
    "    \n",
    "    \n",
    "    y = train['is_duplicate'].values[:]\n",
    "    \n",
    "    X_t = ssp.hstack([\n",
    "        test_question1_tfidf,\n",
    "        test_question2_tfidf,\n",
    "        test_interaction,\n",
    "        test_porter_interaction,\n",
    "        test_jaccard,\n",
    "        test_porter_jaccard,\n",
    "        test_len\n",
    "        ]).tocsr()\n",
    "    \n",
    "    \n",
    "    print X.shape\n",
    "    print X_t.shape\n",
    "    \n",
    "    fromsparsetofile(path + \"x_tfidf.svm\", X, deli1=\" \", deli2=\":\",ytarget=y)\n",
    "    del X\n",
    "    fromsparsetofile(path + \"x_t_tfidf.svm\", X_t, deli1=\" \", deli2=\":\",ytarget=None)\n",
    "    del X_t\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "    \n",
    "    print (\"done!\")      \n",
    "    \n",
    "                     \n",
    "if __name__==\"__main__\":\n",
    "    main()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do banks in dubai offer credit cards on an aed 4000 salary? do banks in dubai offer credit cards on an aed 4000 salary \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "SEED = 2048\n",
    "np.random.seed(SEED)\n",
    "PATH = os.path.expanduser(\"~\") + \"/data/quora/\"\n",
    "\n",
    "train = pd.read_csv(PATH + \"train.csv\", nrows=50000)\n",
    "# test = pd.read_csv(PATH + \"test.csv\")#, nrows=5000)\n",
    "\n",
    "def stem_str(x1,stemmer=SnowballStemmer('english')):\n",
    "    try:\n",
    "        x = text.re.sub(\"[^a-zA-Z0-9]\",\" \", x1)\n",
    "        x = (\" \").join([stemmer.stem(z) for z in x.split(\" \")])\n",
    "        x = \" \".join(x.split())\n",
    "    except: print x1, x\n",
    "    return x\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# print ('Generate porter')\n",
    "# train['question1_porter'] = train['question1'].astype(str).apply(lambda x: stem_str(x.lower(),snowball))\n",
    "# test['question1_porter'] = test['question1'].astype(str).apply(lambda x: stem_str(x.lower(),porter))\n",
    "train['question2_porter'] = train['question2'].astype(str).apply(lambda x: stem_str(x.lower(),porter))\n",
    "# test['question2_porter'] = test['question2'].astype(str).apply(lambda x: stem_str(x.lower(),porter))\n",
    "\n",
    "x= 'do banks in dubai offer credit cards on an aed 4000 salary'\n",
    "x = (\" \").join([snowball.stem(z) for z in x.split(\" \")])\n",
    "x = \" \".join(x.split())\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149596, 149596)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(PATH+'train.csv')\n",
    "pos = df[df.is_duplicate==1]\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "g = nx.Graph()\n",
    "g.add_nodes_from(pos.question1)\n",
    "g.add_nodes_from(pos.question2)\n",
    "edges = list(pos[['question1','question2']].to_records(index=False))\n",
    "g.add_edges_from(edges)\n",
    "len(set(pos.question1) | set(pos.question2)), g.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFCCAYAAAApAA5wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naLbptwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109406cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with block handles a deprecation warning that occurs inside nx.draw_networkx\n",
    "import warnings\n",
    "cc = filter(lambda x : (len(x) > 3), \n",
    "            nx.connected_component_subgraphs(g))\n",
    "# g1 = next(cc)\n",
    "# g1.nodes()\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    nx.draw_circular(cc[2], with_labels=True, alpha=0.5, font_size=8)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How can I be less self conscious?',\n",
       " 'How to be less self-conscious?',\n",
       " 'How can I become less self conscious and insecure?',\n",
       " 'How do I become less conscious of myself around other people?',\n",
       " 'How can be less self-conscious?']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for i in cc[0]:\n",
    "    l.append(i)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named gensim",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a0cf2f0a8d90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfuzzywuzzy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfuzz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named gensim"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Detecting duplicate quora questions\n",
    "feature engineering\n",
    "@author: Abhishek Thakur\n",
    "\"\"\"\n",
    "\n",
    "import cPickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from nltk import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)\n",
    "\n",
    "\n",
    "def norm_wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return norm_model.wmdistance(s1, s2)\n",
    "\n",
    "\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "\n",
    "data = pd.read_csv('data/quora_duplicate_questions.tsv', sep='\\t')\n",
    "data = data.drop(['id', 'qid1', 'qid2'], axis=1)\n",
    "\n",
    "\n",
    "data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
    "data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
    "data['diff_len'] = data.len_q1 - data.len_q2\n",
    "data['len_char_q1'] = data.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data['len_char_q2'] = data.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\n",
    "data['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))\n",
    "data['common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "data['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data['fuzz_WRatio'] = data.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data['fuzz_partial_ratio'] = data.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data['fuzz_partial_token_set_ratio'] = data.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data['fuzz_token_set_ratio'] = data.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data['fuzz_token_sort_ratio'] = data.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "data['wmd'] = data.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "\n",
    "\n",
    "norm_model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "norm_model.init_sims(replace=True)\n",
    "data['norm_wmd'] = data.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)\n",
    "\n",
    "question1_vectors = np.zeros((data.shape[0], 300))\n",
    "error_count = 0\n",
    "\n",
    "for i, q in tqdm(enumerate(data.question1.values)):\n",
    "    question1_vectors[i, :] = sent2vec(q)\n",
    "\n",
    "question2_vectors  = np.zeros((data.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(data.question2.values)):\n",
    "    question2_vectors[i, :] = sent2vec(q)\n",
    "\n",
    "data['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "data['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]\n",
    "\n",
    "cPickle.dump(question1_vectors, open('data/q1_w2v.pkl', 'wb'), -1)\n",
    "cPickle.dump(question2_vectors, open('data/q2_w2v.pkl', 'wb'), -1)\n",
    "\n",
    "data.to_csv('data/quora_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_ques_df[\"num_of_words\"] = all_ques_df[\"questions\"].apply(lambda x : len(str(x).split()))\n",
    "all_ques_df[\"num_of_chars\"] = all_ques_df[\"questions\"].apply(lambda x : len(str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, ngrams\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def get_unigrams(que):\n",
    "    return [word for word in word_tokenize(que.lower()) if word not in eng_stopwords]\n",
    "\n",
    "def get_common_unigrams(row):\n",
    "    # q1 & q2\n",
    "    return len( set(row[\"unigrams_ques1\"]).intersection(set(row[\"unigrams_ques2\"])))\n",
    "\n",
    "def get_common_unigram_ratio(row):\n",
    "    # q1 | q2\n",
    "    return float(row[\"unigrams_common_count\"]) / max( len(set(row[\"unigrams_ques1\"]\n",
    "                                            ).union(set(row[\"unigrams_ques2\"]))), 1)\n",
    "\n",
    "df[\"unigrams_ques1\"] = df['question1'].apply(lambda x: get_unigrams(str(x)))\n",
    "df[\"unigrams_ques2\"] = df['question2'].apply(lambda x: get_unigrams(str(x)))\n",
    "df[\"unigrams_common_count\"] = df.apply(lambda row: get_common_unigrams(row),axis=1)\n",
    "df[\"unigrams_common_ratio\"] = df.apply(lambda row: get_common_unigram_ratio(row), axis=1)\n",
    "\n",
    "def get_bigrams(que):\n",
    "    return [i for i in ngrams(que, 2)]\n",
    "\n",
    "def get_common_bigrams(row):\n",
    "    return len( set(row[\"bigrams_ques1\"]).intersection(set(row[\"bigrams_ques2\"])) )\n",
    "\n",
    "def get_common_bigram_ratio(row):\n",
    "    return float(row[\"bigrams_common_count\"]) / max(len( set(row[\"bigrams_ques1\"]\n",
    "                                            ).union(set(row[\"bigrams_ques2\"]))), 1)\n",
    "\n",
    "df[\"bigrams_ques1\"] = df[\"unigrams_ques1\"].apply(lambda x: get_bigrams(x))\n",
    "df[\"bigrams_ques2\"] = df[\"unigrams_ques2\"].apply(lambda x: get_bigrams(x)) \n",
    "df[\"bigrams_common_count\"] = df.apply(lambda row: get_common_bigrams(row),axis=1)\n",
    "df[\"bigrams_common_ratio\"] = df.apply(lambda row: get_common_bigram_ratio(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction(row):\n",
    "    que1 = str(row['question1'])\n",
    "    que2 = str(row['question2'])\n",
    "    out_list = []\n",
    "    # get unigram features #\n",
    "    unigrams_que1 = [word for word in que1.lower().split() if word not in eng_stopwords]\n",
    "    unigrams_que2 = [word for word in que2.lower().split() if word not in eng_stopwords]\n",
    "    common_unigrams_len = len(set(unigrams_que1).intersection(set(unigrams_que2)))\n",
    "    common_unigrams_ratio = float(common_unigrams_len) / max(len(set(unigrams_que1).union(set(unigrams_que2))),1)\n",
    "    out_list.extend([common_unigrams_len, common_unigrams_ratio])\n",
    "\n",
    "    # get bigram features #\n",
    "    bigrams_que1 = [i for i in ngrams(unigrams_que1, 2)]\n",
    "    bigrams_que2 = [i for i in ngrams(unigrams_que2, 2)]\n",
    "    common_bigrams_len = len(set(bigrams_que1).intersection(set(bigrams_que2)))\n",
    "    common_bigrams_ratio = float(common_bigrams_len) / max(len(set(bigrams_que1).union(set(bigrams_que2))),1)\n",
    "    out_list.extend([common_bigrams_len, common_bigrams_ratio])\n",
    "\n",
    "    # get trigram features #\n",
    "    trigrams_que1 = [i for i in ngrams(unigrams_que1, 3)]\n",
    "    trigrams_que2 = [i for i in ngrams(unigrams_que2, 3)]\n",
    "    common_trigrams_len = len(set(trigrams_que1).intersection(set(trigrams_que2)))\n",
    "    common_trigrams_ratio = float(common_trigrams_len) / max(len(set(trigrams_que1).union(set(trigrams_que2))),1)\n",
    "    out_list.extend([common_trigrams_len, common_trigrams_ratio])\n",
    "    return out_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_dup = train_X[train_y==1]\n",
    "train_X_non_dup = train_X[train_y==0]\n",
    "\n",
    "train_X = np.vstack([train_X_non_dup, train_X_dup, train_X_non_dup, train_X_non_dup])\n",
    "train_y = np.array([0]*train_X_non_dup.shape[0] + [1]*train_X_dup.shape[0] + [0]*train_X_non_dup.shape[0] + [0]*train_X_non_dup.shape[0])\n",
    "del train_X_dup\n",
    "del train_X_non_dup\n",
    "print(\"Mean target rate : \",train_y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224308</th>\n",
       "      <td>224308</td>\n",
       "      <td>168427</td>\n",
       "      <td>214663</td>\n",
       "      <td>How can I transfer all my Google drive and Gma...</td>\n",
       "      <td>How can I transfer contacts from a Lumia 520 t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331061</th>\n",
       "      <td>331061</td>\n",
       "      <td>168427</td>\n",
       "      <td>57325</td>\n",
       "      <td>How can I transfer all my Google drive and Gma...</td>\n",
       "      <td>How many Gmail accounts can I create with one ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341566</th>\n",
       "      <td>341566</td>\n",
       "      <td>168427</td>\n",
       "      <td>383046</td>\n",
       "      <td>How can I transfer all my Google drive and Gma...</td>\n",
       "      <td>What will my limitation of use be if I purchas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "224308  224308  168427  214663   \n",
       "331061  331061  168427   57325   \n",
       "341566  341566  168427  383046   \n",
       "\n",
       "                                                question1  \\\n",
       "224308  How can I transfer all my Google drive and Gma...   \n",
       "331061  How can I transfer all my Google drive and Gma...   \n",
       "341566  How can I transfer all my Google drive and Gma...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "224308  How can I transfer contacts from a Lumia 520 t...             0  \n",
       "331061  How many Gmail accounts can I create with one ...             0  \n",
       "341566  What will my limitation of use be if I purchas...             0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['question1']=='How can I transfer all my Google drive and Gmail data to a different account?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping  = {}\n",
    "df[\"qmax\"] = df.apply( lambda row: max(mapping.setdefault(row[\"question1\"], len(mapping)), \n",
    "                                       mapping.setdefault(row[\"question2\"], len(mapping))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6750</th>\n",
       "      <td>6750</td>\n",
       "      <td>13212</td>\n",
       "      <td>13213</td>\n",
       "      <td>why do you think you are special</td>\n",
       "      <td>why do you think you are special</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23693</th>\n",
       "      <td>23693</td>\n",
       "      <td>44353</td>\n",
       "      <td>44354</td>\n",
       "      <td>what is wrong with this solution</td>\n",
       "      <td>what is wrong with this solution</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30851</th>\n",
       "      <td>30851</td>\n",
       "      <td>56920</td>\n",
       "      <td>56921</td>\n",
       "      <td>what is it like to be gay in hong kong</td>\n",
       "      <td>what is it like to be gay in hong kong</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61404</th>\n",
       "      <td>61404</td>\n",
       "      <td>107213</td>\n",
       "      <td>107214</td>\n",
       "      <td>what is it like to meet larry page</td>\n",
       "      <td>what is it like to meet larry page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78271</th>\n",
       "      <td>78271</td>\n",
       "      <td>133495</td>\n",
       "      <td>133496</td>\n",
       "      <td>i am 17 now how can i earn my first house or l...</td>\n",
       "      <td>i am 17 now how can i earn my first house or l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103525</th>\n",
       "      <td>103525</td>\n",
       "      <td>24587</td>\n",
       "      <td>171095</td>\n",
       "      <td>what is original jurisdiction</td>\n",
       "      <td>what is original jurisdiction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121182</th>\n",
       "      <td>121182</td>\n",
       "      <td>196408</td>\n",
       "      <td>196409</td>\n",
       "      <td>what is the name of this instrument</td>\n",
       "      <td>what is the name of this instrument</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143641</th>\n",
       "      <td>143641</td>\n",
       "      <td>227528</td>\n",
       "      <td>227529</td>\n",
       "      <td>what is the worst excuse you have ever heard</td>\n",
       "      <td>what is the worst excuse you have ever heard</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154513</th>\n",
       "      <td>154513</td>\n",
       "      <td>242289</td>\n",
       "      <td>242290</td>\n",
       "      <td>what type of nikes are these</td>\n",
       "      <td>what type of nikes are these</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158473</th>\n",
       "      <td>158473</td>\n",
       "      <td>247561</td>\n",
       "      <td>247562</td>\n",
       "      <td>what is the name of this song</td>\n",
       "      <td>what is the name of this song</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172120</th>\n",
       "      <td>172120</td>\n",
       "      <td>265841</td>\n",
       "      <td>265842</td>\n",
       "      <td>what is the best way to advertise online</td>\n",
       "      <td>what is the best way to advertise online</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174071</th>\n",
       "      <td>174071</td>\n",
       "      <td>268374</td>\n",
       "      <td>268375</td>\n",
       "      <td>who is this girl</td>\n",
       "      <td>who is this girl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182820</th>\n",
       "      <td>182820</td>\n",
       "      <td>279718</td>\n",
       "      <td>68960</td>\n",
       "      <td>do you think i have ocd</td>\n",
       "      <td>do you think i have ocd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190035</th>\n",
       "      <td>190035</td>\n",
       "      <td>288991</td>\n",
       "      <td>39099</td>\n",
       "      <td>what does my birth chart say about me</td>\n",
       "      <td>what does my birth chart say about me</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192380</th>\n",
       "      <td>192380</td>\n",
       "      <td>292009</td>\n",
       "      <td>292010</td>\n",
       "      <td>i have scored 500 marks in neet can i get admi...</td>\n",
       "      <td>i have scored 500 marks in neet can i get admi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205866</th>\n",
       "      <td>205866</td>\n",
       "      <td>309141</td>\n",
       "      <td>309142</td>\n",
       "      <td>how do i teach my children about english spell...</td>\n",
       "      <td>how do i teach my children about english spell...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211669</th>\n",
       "      <td>211669</td>\n",
       "      <td>316563</td>\n",
       "      <td>316564</td>\n",
       "      <td>why in katy perry -part of me song the watch g...</td>\n",
       "      <td>why in katy perry -part of me song the watch g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220517</th>\n",
       "      <td>220517</td>\n",
       "      <td>327627</td>\n",
       "      <td>327628</td>\n",
       "      <td>does he like me</td>\n",
       "      <td>does he like me</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236250</th>\n",
       "      <td>236250</td>\n",
       "      <td>347125</td>\n",
       "      <td>347126</td>\n",
       "      <td>if you were a farmer what would you grow</td>\n",
       "      <td>if you were a farmer what would you grow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240964</th>\n",
       "      <td>240964</td>\n",
       "      <td>352894</td>\n",
       "      <td>352895</td>\n",
       "      <td>what is the title of this song</td>\n",
       "      <td>what is the title of this song</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251464</th>\n",
       "      <td>251464</td>\n",
       "      <td>365580</td>\n",
       "      <td>365581</td>\n",
       "      <td>how would a teacher be held liable for an inju...</td>\n",
       "      <td>how would a teacher be held liable for an inju...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252019</th>\n",
       "      <td>252019</td>\n",
       "      <td>366246</td>\n",
       "      <td>366247</td>\n",
       "      <td>if you were the last person on earth what woul...</td>\n",
       "      <td>if you were the last person on earth what woul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254962</th>\n",
       "      <td>254962</td>\n",
       "      <td>369789</td>\n",
       "      <td>369790</td>\n",
       "      <td>can someone help me</td>\n",
       "      <td>can someone help me</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272794</th>\n",
       "      <td>272794</td>\n",
       "      <td>391127</td>\n",
       "      <td>391128</td>\n",
       "      <td>what is the story behind this photo</td>\n",
       "      <td>what is the story behind this photo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276854</th>\n",
       "      <td>276854</td>\n",
       "      <td>395845</td>\n",
       "      <td>395846</td>\n",
       "      <td>what is the meaning of this</td>\n",
       "      <td>what is the meaning of this</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285520</th>\n",
       "      <td>285520</td>\n",
       "      <td>405956</td>\n",
       "      <td>405957</td>\n",
       "      <td>what is wrong here</td>\n",
       "      <td>what is wrong here</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308063</th>\n",
       "      <td>308063</td>\n",
       "      <td>431849</td>\n",
       "      <td>431850</td>\n",
       "      <td>can someone translate this in english</td>\n",
       "      <td>can someone translate this in english</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310728</th>\n",
       "      <td>310728</td>\n",
       "      <td>269554</td>\n",
       "      <td>434889</td>\n",
       "      <td>what is the single most effective piece of fin...</td>\n",
       "      <td>what is the single most effective piece of fin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316633</th>\n",
       "      <td>316633</td>\n",
       "      <td>441717</td>\n",
       "      <td>441718</td>\n",
       "      <td>how is this possible</td>\n",
       "      <td>how is this possible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347129</th>\n",
       "      <td>347129</td>\n",
       "      <td>475567</td>\n",
       "      <td>215030</td>\n",
       "      <td>what is the genre of this song</td>\n",
       "      <td>what is the genre of this song</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355138</th>\n",
       "      <td>355138</td>\n",
       "      <td>227529</td>\n",
       "      <td>484352</td>\n",
       "      <td>what is the worst excuse you have ever heard</td>\n",
       "      <td>what is the worst excuse you have ever heard</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365306</th>\n",
       "      <td>365306</td>\n",
       "      <td>495409</td>\n",
       "      <td>495410</td>\n",
       "      <td>why am i so slow</td>\n",
       "      <td>why am i so slow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381782</th>\n",
       "      <td>381782</td>\n",
       "      <td>513543</td>\n",
       "      <td>513544</td>\n",
       "      <td>what is the difference between the two chinese...</td>\n",
       "      <td>what is the difference between the two chinese...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395473</th>\n",
       "      <td>395473</td>\n",
       "      <td>528457</td>\n",
       "      <td>528458</td>\n",
       "      <td>what is the answer to this iq test question</td>\n",
       "      <td>what is the answer to this iq test question</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398714</th>\n",
       "      <td>398714</td>\n",
       "      <td>531956</td>\n",
       "      <td>531957</td>\n",
       "      <td>what is your favorite john carpenter movie</td>\n",
       "      <td>what is your favorite john carpenter movie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399243</th>\n",
       "      <td>399243</td>\n",
       "      <td>532492</td>\n",
       "      <td>280412</td>\n",
       "      <td>what is the iupac name of this compound</td>\n",
       "      <td>what is the iupac name of this compound</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "6750      6750   13212   13213   \n",
       "23693    23693   44353   44354   \n",
       "30851    30851   56920   56921   \n",
       "61404    61404  107213  107214   \n",
       "78271    78271  133495  133496   \n",
       "103525  103525   24587  171095   \n",
       "121182  121182  196408  196409   \n",
       "143641  143641  227528  227529   \n",
       "154513  154513  242289  242290   \n",
       "158473  158473  247561  247562   \n",
       "172120  172120  265841  265842   \n",
       "174071  174071  268374  268375   \n",
       "182820  182820  279718   68960   \n",
       "190035  190035  288991   39099   \n",
       "192380  192380  292009  292010   \n",
       "205866  205866  309141  309142   \n",
       "211669  211669  316563  316564   \n",
       "220517  220517  327627  327628   \n",
       "236250  236250  347125  347126   \n",
       "240964  240964  352894  352895   \n",
       "251464  251464  365580  365581   \n",
       "252019  252019  366246  366247   \n",
       "254962  254962  369789  369790   \n",
       "272794  272794  391127  391128   \n",
       "276854  276854  395845  395846   \n",
       "285520  285520  405956  405957   \n",
       "308063  308063  431849  431850   \n",
       "310728  310728  269554  434889   \n",
       "316633  316633  441717  441718   \n",
       "347129  347129  475567  215030   \n",
       "355138  355138  227529  484352   \n",
       "365306  365306  495409  495410   \n",
       "381782  381782  513543  513544   \n",
       "395473  395473  528457  528458   \n",
       "398714  398714  531956  531957   \n",
       "399243  399243  532492  280412   \n",
       "\n",
       "                                                question1  \\\n",
       "6750                     why do you think you are special   \n",
       "23693                    what is wrong with this solution   \n",
       "30851              what is it like to be gay in hong kong   \n",
       "61404                  what is it like to meet larry page   \n",
       "78271   i am 17 now how can i earn my first house or l...   \n",
       "103525                      what is original jurisdiction   \n",
       "121182                what is the name of this instrument   \n",
       "143641       what is the worst excuse you have ever heard   \n",
       "154513                       what type of nikes are these   \n",
       "158473                      what is the name of this song   \n",
       "172120           what is the best way to advertise online   \n",
       "174071                                   who is this girl   \n",
       "182820                            do you think i have ocd   \n",
       "190035             what does my birth chart say about me    \n",
       "192380  i have scored 500 marks in neet can i get admi...   \n",
       "205866  how do i teach my children about english spell...   \n",
       "211669  why in katy perry -part of me song the watch g...   \n",
       "220517                                   does he like me    \n",
       "236250           if you were a farmer what would you grow   \n",
       "240964                     what is the title of this song   \n",
       "251464  how would a teacher be held liable for an inju...   \n",
       "252019  if you were the last person on earth what woul...   \n",
       "254962                                can someone help me   \n",
       "272794                what is the story behind this photo   \n",
       "276854                        what is the meaning of this   \n",
       "285520                                 what is wrong here   \n",
       "308063              can someone translate this in english   \n",
       "310728  what is the single most effective piece of fin...   \n",
       "316633                               how is this possible   \n",
       "347129                     what is the genre of this song   \n",
       "355138       what is the worst excuse you have ever heard   \n",
       "365306                                   why am i so slow   \n",
       "381782  what is the difference between the two chinese...   \n",
       "395473        what is the answer to this iq test question   \n",
       "398714         what is your favorite john carpenter movie   \n",
       "399243            what is the iupac name of this compound   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "6750                     why do you think you are special             0  \n",
       "23693                    what is wrong with this solution             0  \n",
       "30851              what is it like to be gay in hong kong             0  \n",
       "61404                  what is it like to meet larry page             0  \n",
       "78271   i am 17 now how can i earn my first house or l...             0  \n",
       "103525                      what is original jurisdiction             0  \n",
       "121182                what is the name of this instrument             0  \n",
       "143641       what is the worst excuse you have ever heard             0  \n",
       "154513                       what type of nikes are these             0  \n",
       "158473                      what is the name of this song             0  \n",
       "172120           what is the best way to advertise online             0  \n",
       "174071                                   who is this girl             0  \n",
       "182820                            do you think i have ocd             0  \n",
       "190035             what does my birth chart say about me              0  \n",
       "192380  i have scored 500 marks in neet can i get admi...             0  \n",
       "205866  how do i teach my children about english spell...             0  \n",
       "211669  why in katy perry -part of me song the watch g...             0  \n",
       "220517                                   does he like me              0  \n",
       "236250           if you were a farmer what would you grow             0  \n",
       "240964                     what is the title of this song             0  \n",
       "251464  how would a teacher be held liable for an inju...             0  \n",
       "252019  if you were the last person on earth what woul...             0  \n",
       "254962                                can someone help me             0  \n",
       "272794                what is the story behind this photo             0  \n",
       "276854                        what is the meaning of this             0  \n",
       "285520                                 what is wrong here             0  \n",
       "308063              can someone translate this in english             0  \n",
       "310728  what is the single most effective piece of fin...             0  \n",
       "316633                               how is this possible             0  \n",
       "347129                     what is the genre of this song             0  \n",
       "355138       what is the worst excuse you have ever heard             0  \n",
       "365306                                   why am i so slow             0  \n",
       "381782  what is the difference between the two chinese...             0  \n",
       "395473        what is the answer to this iq test question             0  \n",
       "398714         what is your favorite john carpenter movie             0  \n",
       "399243            what is the iupac name of this compound             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "### 找到 q1 = q2 但是label=0的 noise\n",
    "##################################\n",
    "\n",
    "train = pd.read_csv(PATH+'train.csv')\n",
    "\n",
    "punctuation='[\"\\'?,\\.]' # I will replace all these punctuation with ''\n",
    "abbr_dict={\n",
    "    \"what's\":\"what is\",\n",
    "    \"what're\":\"what are\",\n",
    "    \"who's\":\"who is\",\n",
    "    \"who're\":\"who are\",\n",
    "    \"where's\":\"where is\",\n",
    "    \"where're\":\"where are\",\n",
    "    \"when's\":\"when is\",\n",
    "    \"when're\":\"when are\",\n",
    "    \"how's\":\"how is\",\n",
    "    \"how're\":\"how are\",\n",
    "\n",
    "    \"i'm\":\"i am\",\n",
    "    \"we're\":\"we are\",\n",
    "    \"you're\":\"you are\",\n",
    "    \"they're\":\"they are\",\n",
    "    \"it's\":\"it is\",\n",
    "    \"he's\":\"he is\",\n",
    "    \"she's\":\"she is\",\n",
    "    \"that's\":\"that is\",\n",
    "    \"there's\":\"there is\",\n",
    "    \"there're\":\"there are\",\n",
    "\n",
    "    \"i've\":\"i have\",\n",
    "    \"we've\":\"we have\",\n",
    "    \"you've\":\"you have\",\n",
    "    \"they've\":\"they have\",\n",
    "    \"who've\":\"who have\",\n",
    "    \"would've\":\"would have\",\n",
    "    \"not've\":\"not have\",\n",
    "\n",
    "    \"i'll\":\"i will\",\n",
    "    \"we'll\":\"we will\",\n",
    "    \"you'll\":\"you will\",\n",
    "    \"he'll\":\"he will\",\n",
    "    \"she'll\":\"she will\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"they'll\":\"they will\",\n",
    "\n",
    "    \"isn't\":\"is not\",\n",
    "    \"wasn't\":\"was not\",\n",
    "    \"aren't\":\"are not\",\n",
    "    \"weren't\":\"were not\",\n",
    "    \"can't\":\"can not\",\n",
    "    \"couldn't\":\"could not\",\n",
    "    \"don't\":\"do not\",\n",
    "    \"didn't\":\"did not\",\n",
    "    \"shouldn't\":\"should not\",\n",
    "    \"wouldn't\":\"would not\",\n",
    "    \"doesn't\":\"does not\",\n",
    "    \"haven't\":\"have not\",\n",
    "    \"hasn't\":\"has not\",\n",
    "    \"hadn't\":\"had not\",\n",
    "    \"won't\":\"will not\",\n",
    "    punctuation:'',\n",
    "    '\\s+':' ', # replace multi space with one single space\n",
    "    }\n",
    "\n",
    "def process_data(data):\n",
    "    data.question1=data.question1.str.lower() # conver to lower case\n",
    "    data.question2=data.question2.str.lower()\n",
    "    data.question1=data.question1.astype(str)\n",
    "    data.question2=data.question2.astype(str)\n",
    "    data.replace(abbr_dict,regex=True,inplace=True)\n",
    "#     display(data.head(2))\n",
    "    return data\n",
    "\n",
    "df1 = process_data(train)\n",
    "df1[(df1['question1']==df1['question2']) & (df1['is_duplicate']==0) ]"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
